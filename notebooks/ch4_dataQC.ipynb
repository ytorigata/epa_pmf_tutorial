{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddbb0270-cea1-45be-a10b-d0b769375422",
   "metadata": {},
   "source": [
    "# 4. Create Input Files for EPA PMF\n",
    "\n",
    "This notebook will produce two input files for EPA PMF v5.0: concentration file and uncertainty file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad75987f-cd24-4ab9-8709-4fac691ddd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "import sys\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "# set project root\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from src.config import *\n",
    "from src.pmf.input_splitter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58bca9b-9fcd-4d6c-b5ca-673012eff93d",
   "metadata": {},
   "source": [
    "## 4.1. Handle Missing MDL Values\n",
    "\n",
    "When preparing input for PMF analysis, follow these steps to fill in missing MDL (Method Detection Limit) values:\n",
    "\n",
    "- If MDL is reported\n",
    "    - → Use the reported MDL value directly.\n",
    "- If MDL is missing, check the availability of blank samples:\n",
    "    - If 7 or more Field Blanks or Travel Blanks are available\n",
    "        - → Calculate the standard deviation of the blank values and use:\n",
    "        - MDL = 3 × standard deviation of blank\n",
    "    - If fewer than 7 blank values are available\n",
    "        - → Find the minimum positive concentration measured for the analyte, and use:\n",
    "        - MDL = 0.5 × minimum positive value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0ab6d9-8330-4e58-b0ca-341d76422297",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2020\n",
    "site_id = 100119\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caadb8c-e683-43ef-b6f6-8c3b1a63e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_map = get_sheets_for_year(year)\n",
    "\n",
    "for key in ['nt', 'pm25']:\n",
    "    csv_path = PROCESSED_DATA_DIR / f\"{year}_{site_id}_{key}.csv\"\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        csv_path,\n",
    "        index_col=\"sampling_date\",  # column header you wrote out\n",
    "        parse_dates=True,           # turn it into pandas-datetime\n",
    "    )\n",
    "    \n",
    "    df_filled = fill_missing_mdl(df)\n",
    "    df_routine = df_filled[df_filled['sampling_type'] == 'R']\n",
    "\n",
    "    if key == 'nt':\n",
    "        prefix = 'NT_'\n",
    "    elif key == 'ion':\n",
    "        prefix = 'ion_'\n",
    "    else:\n",
    "        prefix = ''\n",
    "\n",
    "    export_analytes(\n",
    "        df_routine,\n",
    "        str(PMF_INPUT_DATA_DIR) + f'/{site_id}_tmp',\n",
    "        prefix=prefix\n",
    "    )\n",
    "\n",
    "# export_analytes(\n",
    "#     str(PROCESSED_DATA_DIR) + f'/{year}_{site_id}_pm25.csv',\n",
    "#     str(PMF_INPUT_DATA_DIR) + f'/{site_id}_tmp',\n",
    "#     prefix=''\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4faad51-28bd-404e-8f10-6dd9df881043",
   "metadata": {},
   "source": [
    "## 4.2. Coverage Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9230df3c-2a57-40fc-a3b9-5787b01547d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_analyte_groups(site_id, base_dir):\n",
    "    \"\"\"\n",
    "    Load PMF input analyte files for a given site and classify them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    site_id : int or str\n",
    "        NAPS site ID used to construct the folder name.\n",
    "    base_dir : Path or str\n",
    "        Root directory where PMF input files are stored.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict of DataFrames:\n",
    "        A dictionary with keys 'metals', 'ions', 'others',\n",
    "        each containing a DataFrame of analytes.\n",
    "    \"\"\"\n",
    "    folder = Path(base_dir) / (str(site_id) + '_tmp')\n",
    "    all_files = sorted(folder.glob(\"*.csv\"))\n",
    "\n",
    "    nt_pm25_files = []\n",
    "    ion_files = []\n",
    "    other_files = []\n",
    "\n",
    "    # Classify files\n",
    "    for file_path in all_files:\n",
    "        fname = file_path.name\n",
    "        if fname.startswith(\"NT_\") or fname == \"PM25.csv\":\n",
    "            nt_pm25_files.append(file_path)\n",
    "        elif fname.startswith(\"ion_\"):\n",
    "            ion_files.append(file_path)\n",
    "        else:\n",
    "            other_files.append(file_path)\n",
    "\n",
    "    # Helper to load and name columns\n",
    "    def load_files(file_list):\n",
    "        dfs = []\n",
    "        for file_path in file_list:\n",
    "            analyte_name = file_path.stem\n",
    "            df = pd.read_csv(file_path, parse_dates=['Date'], index_col='Date')\n",
    "    \n",
    "            # Check for duplicated dates\n",
    "            duplicated_dates = df.index[df.index.duplicated()]\n",
    "            if not duplicated_dates.empty:\n",
    "                print(f\"!!!!! Duplicate dates found in: {file_path.name}\")\n",
    "                print(duplicated_dates)\n",
    "    \n",
    "            # Optionally: keep first or handle them differently\n",
    "            df = df[~df.index.duplicated(keep='first')]\n",
    "    \n",
    "            # Use only the concentration column (assumed to be first)\n",
    "            df = df.iloc[:, [0]]\n",
    "            df.columns = [analyte_name]\n",
    "            dfs.append(df)\n",
    "    \n",
    "        return pd.concat(dfs, axis=1) if dfs else pd.DataFrame()\n",
    "    \n",
    "    return {\n",
    "        'metals': load_files(nt_pm25_files),\n",
    "        'ions': load_files(ion_files),\n",
    "        'others': load_files(other_files),\n",
    "    }\n",
    "\n",
    "dfs_dic = load_analyte_groups(site_id=100119, base_dir=PMF_INPUT_DATA_DIR)\n",
    "\n",
    "df_nt_pm25 = dfs_dic['metals']\n",
    "# df_ions = dfs_dic['ions']\n",
    "df_other = dfs_dic['others']\n",
    "\n",
    "print(df_nt_pm25.columns)\n",
    "# print(df_ions.columns)\n",
    "print(df_other.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2c25d9-fb0c-49a9-bd37-7542392a6bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_nt_pm25.copy()\n",
    "df.index = pd.to_datetime(df.index, errors='coerce')\n",
    "\n",
    "# 1. Assign season labels\n",
    "season_months = {\n",
    "    \"Winter\": [12, 1, 2],\n",
    "    \"Spring\": [3, 4, 5],\n",
    "    \"Summer\": [6, 7, 8],\n",
    "    \"Fall\":   [9, 10, 11]\n",
    "}\n",
    "\n",
    "def assign_year_season(date):\n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    for season, months in season_months.items():\n",
    "        if month in months:\n",
    "            if season == \"Winter\" and month == 12:\n",
    "                year += 1\n",
    "            return f\"{year}-{season}\"\n",
    "\n",
    "# 2. Function to count calendar days in a season\n",
    "def count_calendar_days(year, season):\n",
    "    months = season_months[season]\n",
    "    days = 0\n",
    "    for m in months:\n",
    "        y = year - 1 if m == 12 else year\n",
    "        month_end = pd.Timestamp(year=y, month=m, day=1) + MonthEnd(1)\n",
    "        days += month_end.day\n",
    "    return days\n",
    "\n",
    "# 3.\n",
    "def get_freq(site_id, year):\n",
    "    \"\"\" Return measurement frequency based on the year and site. \"\"\"\n",
    "    if site_id == 100119:\n",
    "        return 3 if year < 2013 else 6\n",
    "    elif site_id == 60211:\n",
    "        return 3\n",
    "    else:\n",
    "        return 0  # or raise ValueError\n",
    "\n",
    "# ---- MAIN CALCULATION ----\n",
    "\n",
    "df['year_season'] = df.index.map(assign_year_season)\n",
    "analytes = [col for col in df.columns if col not in ['year_season'] and not col.endswith('-MDL') and not col.endswith('-VFlag')]\n",
    "\n",
    "# 4. Actual counts per season\n",
    "actual_counts = df.groupby('year_season')[analytes].count()\n",
    "\n",
    "# 5. Calculate ideal counts per season\n",
    "ideal_counts = []\n",
    "for label in actual_counts.index:\n",
    "    year_str, season = label.split('-')\n",
    "    year = int(year_str)\n",
    "    days = count_calendar_days(year, season)\n",
    "    freq = get_freq(site_id, year)\n",
    "    ideal = days / freq if freq > 0 else np.nan\n",
    "    ideal_counts.append(ideal)\n",
    "\n",
    "# 6. Calculate coverage\n",
    "ideal_series = pd.Series(ideal_counts, index=actual_counts.index, name='ideal')\n",
    "coverage_df = actual_counts.divide(ideal_series, axis=0).clip(upper=1.0)\n",
    "\n",
    "# 7. Sort seasons chronologically\n",
    "season_order = {'Winter': 1, 'Spring': 2, 'Summer': 3, 'Fall': 4}\n",
    "coverage_df['sort_key'] = coverage_df.index.map(lambda s: (int(s.split('-')[0]), season_order[s.split('-')[1]]))\n",
    "coverage_df = coverage_df.sort_values('sort_key').drop(columns='sort_key')\n",
    "\n",
    "# 8. Categorical heatmap (red/yellow/green for <50%, 50–75%, ≥75%)\n",
    "# Multiply values by 100 to convert to percentage\n",
    "coverage_pct = coverage_df * 100\n",
    "\n",
    "# Define bins in percentage\n",
    "bins = [0, 50, 75, 100]\n",
    "\n",
    "# Use a single hue (blue), darker = higher coverage\n",
    "cmap = sns.light_palette(\"#006cd1\", n_colors=3)\n",
    "\n",
    "# Bin for coloring\n",
    "categorized = coverage_pct.apply(lambda col: np.digitize(col.fillna(0), bins) - 1)\n",
    "\n",
    "# Plot setup\n",
    "fig = plt.figure(figsize=(len(coverage_pct.columns) * 0.4 + 2, len(coverage_pct) * 0.4 + 2))\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[20, 1])\n",
    "ax = fig.add_subplot(gs[0])\n",
    "\n",
    "# Disable actual color bar, so it doesn't interfere\n",
    "sns.heatmap(\n",
    "    categorized,\n",
    "    cmap=cmap,\n",
    "    annot=coverage_pct.round().astype(int),\n",
    "    fmt='d',\n",
    "    linewidths=0.5,\n",
    "    linecolor='black',\n",
    "    cbar=False,  # We will use custom legend instead\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# Custom color legend (patches)\n",
    "legend_labels = ['<50%', '50–75%', '≥75%']\n",
    "legend_colors = cmap\n",
    "patches = [mpatches.Patch(color=legend_colors[i], label=legend_labels[i]) for i in range(3)]\n",
    "\n",
    "# Place legend just below the heatmap\n",
    "ax.legend(\n",
    "    handles=patches,\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, -0.5),\n",
    "    ncol=3,\n",
    "    title=\"Coverage (%)\",\n",
    "    frameon=False\n",
    ")\n",
    "\n",
    "# Axis labels and title\n",
    "ax.set_title(\"Seasonal Measurement Coverage per Analyte\")\n",
    "ax.set_xlabel(\"Analyte\")\n",
    "ax.set_ylabel(\"Season\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(coverage_dir + f'/{site_id}_seasonal_75_{target_to_plot}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506be25-74f4-4ba9-ae49-384a6cd475a7",
   "metadata": {},
   "source": [
    "## 4.3. Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b30986-abbf-4c29-aace-c494f4164bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.to_datetime('2020-01-01')\n",
    "end_date = pd.to_datetime('2020-12-31')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e22d5c-b981-498f-b26e-345420e4b3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_analyte_columns(df, priority_analytes=['PM25']):\n",
    "    \"\"\"\n",
    "    Reorder the columns of a DataFrame to prioritize specific analytes,\n",
    "    followed by the rest in alphabetical order.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame with analytes as columns and datetime as index\n",
    "    - priority_analytes: list of analyte names to place at the front\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with reordered columns\n",
    "    \"\"\"\n",
    "    # Keep only priority analytes that are actually in the DataFrame\n",
    "    existing_priority = [a for a in priority_analytes if a in df.columns]\n",
    "    \n",
    "    # Get the rest of the analytes in alphabetical order\n",
    "    remaining = sorted([a for a in df.columns if a not in existing_priority])\n",
    "    \n",
    "    # Combine into the new order\n",
    "    new_order = existing_priority + remaining\n",
    "    \n",
    "    # Reorder and return\n",
    "    return df[new_order]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2ec34a-03ae-4c5b-b2c4-53e8fc10f84b",
   "metadata": {},
   "source": [
    "### EPA PMF Uncertainty Rules (Plain Language)\n",
    "\n",
    "When you build inputs for EPA PMF, each measurement needs both a **concentration** and an **uncertainty**. Here’s how we handle different cases:\n",
    "\n",
    "1. **Good measurements** (concentration above the MDL):  \n",
    "   - We calculate uncertainty as  \n",
    "     $\n",
    "     sqrt{(0.1 \\times \\text{Concentration})^2 + (0.5 \\times \\text{MDL})^2}.\n",
    "     $\n",
    "   - This means “10% of the measured value” combined with “50% of the detection limit.”\n",
    "\n",
    "2. **Below the MDL** (concentration at or below the MDL):  \n",
    "   - We set the concentration to **half the MDL**:  \n",
    "     `Concentration = 0.5 × MDL`  \n",
    "   - We set the uncertainty to **five-sixths of the MDL**:  \n",
    "     `Uncertainty = (5/6) × MDL`\n",
    "\n",
    "3. **Missing values** (no concentration reported):  \n",
    "   - We fill the concentration with the **median** value for that chemical (the middle value when you sort all measurements).  \n",
    "   - We set the uncertainty to **four times** that median.\n",
    "\n",
    "4. **Special “no data” flag** (e.g. your `-999` values):  \n",
    "   - We leave these exactly as they are, so you can always spot them later.\n",
    "\n",
    "This approach gives you realistic error bars for good data, a conservative guess for values below detection, and a consistent way to fill in gaps. Simply paste your concentration and MDL columns into these rules to get the uncertainty column for PMF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339135d-860d-430b-8e15-c26e048d3d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_epa_uncertainty_rules(conc_df: pd.DataFrame, mdl_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Apply EPA PMF rules to generate uncertainty and fill missing values.\n",
    "    - If conc <= MDL: conc = 0.5 * MDL, unc = 5/6 * MDL\n",
    "    - If conc is NaN: conc = species median, unc = 4 * species median\n",
    "    - If conc > MDL: unc = sqrt((0.1*conc)^2 + (0.5*MDL)^2)\n",
    "    - Keep -999 unchanged in both conc and unc\n",
    "    \"\"\"\n",
    "    conc_out = conc_df.copy()\n",
    "    unc_out = pd.DataFrame(index=conc_df.index, columns=conc_df.columns, dtype=float)\n",
    "\n",
    "    for col in conc_df.columns:\n",
    "        conc = conc_df[col]\n",
    "        mdl = mdl_df[col]\n",
    "\n",
    "        # Flags\n",
    "        is_minus999 = conc == -999\n",
    "        is_nan      = conc.isna()\n",
    "        is_below    = (~is_minus999) & (conc <= mdl)\n",
    "        is_above    = (~is_minus999) & (~is_nan) & (conc > mdl)\n",
    "\n",
    "        # Species median (exclude NaN and -999)\n",
    "        median_val = conc[~is_nan & ~is_minus999].median()\n",
    "\n",
    "        # Prepare filled series\n",
    "        conc_filled = conc.copy()\n",
    "        unc_filled  = pd.Series(index=conc.index, dtype=float)\n",
    "\n",
    "        # Rule 1: conc <= MDL\n",
    "        conc_filled[is_below] = 0.5 * mdl[is_below]\n",
    "        unc_filled[is_below]  = (5/6) * mdl[is_below]\n",
    "\n",
    "        # Rule 2: missing conc\n",
    "        conc_filled[is_nan] = median_val\n",
    "        unc_filled[is_nan]  = 4 * median_val\n",
    "\n",
    "        # Rule 3: conc > MDL\n",
    "        unc_filled[is_above] = np.sqrt((0.1 * conc[is_above])**2 + (0.5 * mdl[is_above])**2)\n",
    "\n",
    "        # Rule 4: keep -999\n",
    "        conc_filled[is_minus999] = -999\n",
    "        unc_filled[is_minus999]  = -999\n",
    "\n",
    "        conc_out[col] = conc_filled\n",
    "        unc_out[col]  = unc_filled\n",
    "\n",
    "    return conc_out, unc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91abdb46-054a-4364-b7b4-1fa61a0452f8",
   "metadata": {},
   "source": [
    "The cell below will create the concentration and uncertainty files for input into EPA PMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb7de0-ce8b-4325-b133-0b9c123b24bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new folder for two input files\n",
    "pmf_input_dir = PMF_INPUT_DATA_DIR / Path(str(site_id))\n",
    "os.makedirs(pmf_input_dir, exist_ok=True)\n",
    "\n",
    "# where files are stored\n",
    "folder_name = str(PMF_INPUT_DATA_DIR) + f'/{site_id}_tmp'\n",
    "folder =  Path(folder_name)\n",
    "\n",
    "# Collect series\n",
    "concentration_series = []\n",
    "mdl_series           = []\n",
    "\n",
    "for file_path in sorted(folder.glob(\"*.csv\")):\n",
    "    df = pd.read_csv(file_path, parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "    df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
    "    df = df[(df.index >= start_date) & (df.index <= end_date)]\n",
    "    if df.shape[1] < 2:\n",
    "        continue\n",
    "\n",
    "    analyte = file_path.stem\n",
    "    conc    = df.iloc[:, 0].copy()  # concentration column\n",
    "    mdl     = df.iloc[:, 1].copy()  # MDL column\n",
    "    conc.name = analyte\n",
    "    mdl.name  = analyte\n",
    "\n",
    "    concentration_series.append(conc)\n",
    "    mdl_series.append(mdl)\n",
    "\n",
    "# Combine into DataFrames\n",
    "concentration_df = pd.concat(concentration_series, axis=1)\n",
    "mdl_df           = pd.concat(mdl_series, axis=1)\n",
    "\n",
    "# Apply EPA PMF uncertainty rules\n",
    "concentration_filled, uncertainty_filled = apply_epa_uncertainty_rules(concentration_df, mdl_df)\n",
    "\n",
    "# reorder columns\n",
    "concentration_filled = reorder_analyte_columns(concentration_filled)\n",
    "uncertainty_filled   = reorder_analyte_columns(uncertainty_filled)\n",
    "\n",
    "# Save final CSVs\n",
    "concentration_filled.to_csv(pmf_input_dir / f\"{site_id}_conc.csv\",    index_label=\"Date\")\n",
    "uncertainty_filled .to_csv(pmf_input_dir / f\"{site_id}_unc.csv\",     index_label=\"Date\")\n",
    "\n",
    "# Preview\n",
    "display(concentration_filled.head(3))\n",
    "display(uncertainty_filled.head(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
