{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6653ff6b-78a3-4ac1-9f47-975c961d895b",
   "metadata": {},
   "source": [
    "# 2. Load Measurement Data for Our Target Site\n",
    "\n",
    "Now that we’ve identified our target station (BURNABY SOUTH, NAPS ID: 100119), let’s load the actual measurement data collected at this site.\n",
    "\n",
    "We will use data from **2020 to 2023**.\n",
    "\n",
    "The cell below imports the required packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674a558d-a8e0-4950-9bbd-212e9978ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "# set project root\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from src.config import *\n",
    "from src.data_cleaning import *\n",
    "from src.download_data import *\n",
    "from src.load_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1727b154-bd5f-4670-abcb-eaebce743f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BURNABY SOUTH\n",
    "site_id = 100119\n",
    "\n",
    "years = range(2020, 2024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41d4d9d-7da7-44bc-8ca3-0ac60822d8d0",
   "metadata": {},
   "source": [
    "## 2.1. Download the Data\n",
    "\n",
    "The code below will download the NAPS integrated measurement data from **2020 to 2023**. These files include air pollutant data collected at the **BURNABY SOUTH** station.\n",
    "\n",
    "<details>\n",
    "  <summary><strong>Optional: Manual Download (click to expand)</strong></summary>\n",
    "\n",
    "  If the automatic download fails, you can download the files manually:\n",
    "\n",
    "  1. Visit the [NAPS data portal](https://data-donnees.az.ec.gc.ca/data/air/monitor/national-air-pollution-surveillance-naps-program/)\n",
    "  2. Navigate to:  \n",
    "     `Data-Donnees/` → `2020/` → `IntegratedData-DonneesPonctuelles/` →  \n",
    "     `2020_IntegratedPM2.5-PM2.5Ponctuelles.zip`\n",
    "  3. Save the file to: `../data/raw/integrated/`\n",
    "  4. Repeat steps 2–3 for the years **2021, 2022, and 2023**.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bacbd4a-3412-42fe-81d7-05489258cb6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "download_integrated_pm25()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a013f80-b824-4037-b5de-b5ca5245d2b2",
   "metadata": {},
   "source": [
    "## 2.2. Unzip the Downloaded Files\n",
    "\n",
    "After downloading the data, we need to unzip the files for each year from **2020 to 2023**.\n",
    "\n",
    "The cell below will extract all ZIP files to the `../data/raw/integrated_data/` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad8b70e-163f-4f87-935d-fd5c4a364a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list from 2020 (inclusive) to 2024 (exclusive)\n",
    "years = range(2020, 2024)\n",
    "\n",
    "for year in years: \n",
    "    zip_path = f'../data/raw/integrated/{year}_IntegratedPM2.5-PM2.5Ponctuelles.zip'\n",
    "    extract_to = '../data/raw/integrated_data'\n",
    "\n",
    "    if os.path.exists(zip_path):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(f\"Extracted: {zip_path}\")\n",
    "    else:\n",
    "        print(f\"File not found: {zip_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df20dd3a-73f2-4d8c-b2f9-52c3656e0648",
   "metadata": {},
   "source": [
    "## 2.3. Check the File and Data\n",
    "\n",
    "Before loading the data into Python, let’s take a moment to open one of the unzipped files manually and see what it contains.\n",
    "\n",
    "You can use any spreadsheet program to open the file. I recommend starting with the **2020** file:\n",
    "\n",
    "`../data/raw/integrated/2020_IntegratedPM2.5-PM2.5Ponctuelles/S100119_PM25_2020_EN.xlsx`\n",
    "\n",
    "### What You’ll See and Explore\n",
    "\n",
    "The file contains **integrated PM2.5 speciation data**, organized into multiple worksheets by measurement type.\n",
    "\n",
    "- The `Station Info` sheet includes metadata such as `Sampling Frequency` for each site and year.\n",
    "- We will focus on the `Metals_ICPMS (Near-Total)` sheet, which contains time series data for various metal species.\n",
    "- Use the `Sampling Type` column to distinguish routine measurements from field blanks.\n",
    "- Missing values are common in some rows and should be expected.\n",
    "\n",
    "As you explore the data:\n",
    "\n",
    "- Check which parameters were measured and in what units\n",
    "- Note how frequently data was collected (e.g., every 3 or 6 days)\n",
    "- Look for patterns, missing values, or anomalies\n",
    "\n",
    "> **Try this:**\n",
    "> \n",
    "> Open the **2021** file as well and compare it with 2020.\n",
    "> \n",
    "> `../data/raw/integrated/2021_IntegratedPM2.5-PM2.5Ponctuelles/S100119_PM25_2021_EN.xlsx`\n",
    ">\n",
    "> You’ll notice the set of worksheets isn’t identical — Which sheets are new or missing? \n",
    "\n",
    "Once you're familiar with the structure, we’ll load the files into Python in the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ba8cd3-7c37-451b-b79c-af389b0f58b1",
   "metadata": {},
   "source": [
    "## 2.4. Load and Preview the Measurement Data\n",
    "\n",
    "The code below loads the measurement data from the **Metals_ICPMS (Near-Total)** worksheet (and others, if needed) for each year from 2020 to 2023.\n",
    "\n",
    "Each Excel file contains multiple worksheets, and the structure may vary slightly from year to year. The function `get_sheets_for_year(year)` helps map consistent sheet labels (e.g., `'nt'`) to the correct worksheet name for each year.\n",
    "\n",
    "For each year:\n",
    "\n",
    "- We load the data from the selected sheets (e.g., `'Metals_ICPMS (Near-Total)'`)\n",
    "- Skip the first 9 rows (which contain metadata)\n",
    "- Use the 10th row as the header (column names)\n",
    "- Display the first 3 rows of each loaded sheet to give you a preview\n",
    "\n",
    "This helps confirm that the file paths, sheet names, and data structure are correct before we proceed to combine and analyze the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c225213-2d7e-4828-b27f-f70dfbb84aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    print(f\"\\n--- Year {year} ---\")\n",
    "    \n",
    "    sheet_map = get_sheets_for_year(year)\n",
    "    # shows which sheets exist this year\n",
    "    pprint(sheet_map)\n",
    "\n",
    "    for key in sheet_map:  # e.g. 'nt', 'ws', ...\n",
    "        df = load_target_sheet(year, site_id, key)\n",
    "        print(f\"\\n▶ {key} · {len(df):,} rows\")\n",
    "        display(df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b9d358-208e-4901-b195-e7af1bb46f6a",
   "metadata": {},
   "source": [
    "## 2.5. Clean and Save the Data\n",
    "\n",
    "Before saving the measurement data, we’ll simplify each sheet:\n",
    "\n",
    "- Drop columns ending in `-VFlag`, which contain validation flags that are not useful for our analysis\n",
    "- Rename key columns (e.g., `Sampling Date` → `sampling_date`)\n",
    "- Shorten analyte column names by keeping only the abbreviation in parentheses (e.g., `Selenium (Se)` → `Se`)\n",
    "\n",
    "The cleaned data will be saved as CSV files in the `data/processed/` folder, one file per year and data type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642de00-ccc1-4456-9508-dc7cfd405dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    print(f\"\\nSaving cleaned data for {year}...\")\n",
    "\n",
    "    sheet_map = get_sheets_for_year(year)\n",
    "\n",
    "    for key in sheet_map:\n",
    "        df = load_target_sheet(year, site_id, key)\n",
    "\n",
    "        df_clean = (\n",
    "            df.pipe(drop_vflag_cols)\n",
    "            .pipe(rename_cols)\n",
    "            .pipe(normalize_analyte_names)\n",
    "        )\n",
    "\n",
    "        df_clean = df_clean.set_index('sampling_date')\n",
    "\n",
    "        out_path = PROCESSED_DATA_DIR / f\"{year}_{site_id}_{key}.csv\"\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        df_clean.to_csv(\n",
    "            out_path,\n",
    "            index_label=\"sampling_date\",\n",
    "            date_format=\"%Y-%m-%d\"\n",
    "        )\n",
    "\n",
    "        print(f\"Saved: {out_path.name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9603b6f8-2596-434b-852c-c99b8405c962",
   "metadata": {},
   "source": [
    "The cell below will save PM2.5 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e3593-604c-428b-b9aa-91d48b718834",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    print(f\"\\nSaving cleaned PM2.5 data for {year}...\")\n",
    "    \n",
    "    pm25_df = load_target_sheet(year, site_id, 'pm25')\n",
    "    \n",
    "    # Keep only first PM2.5 and MDL\n",
    "    pm25_subset = pm25_df[[\n",
    "        'NAPS Site ID', 'Sampling Date', 'Sample Type',\n",
    "        'PM2.5', 'PM2.5-MDL'\n",
    "    ]].copy()\n",
    "    \n",
    "\n",
    "    pm25_subset = convert_micro_to_nano(pm25_subset)\n",
    "    \n",
    "    df_clean = (\n",
    "        pm25_subset.pipe(rename_cols)\n",
    "                   .pipe(normalize_analyte_names)\n",
    "                   .rename(columns={'PM2.5': 'PM25', 'PM2.5-MDL': 'PM25-MDL'})\n",
    "                   .set_index(\"sampling_date\")\n",
    "    )\n",
    "\n",
    "    out_path = PROCESSED_DATA_DIR / f\"{year}_{site_id}_pm25.csv\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    df_clean.to_csv(\n",
    "            out_path,\n",
    "            index_label=\"sampling_date\",\n",
    "            date_format=\"%Y-%m-%d\"\n",
    "    )\n",
    "\n",
    "    display(df_clean.head(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
